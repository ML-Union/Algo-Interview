![](https://tva1.sinaimg.cn/large/008eGmZEly1gnsoe1b3vnj31c60qogn0.jpg)

## Deep Crossing

#### 请简述Deep Crossing的网络结构。
Deep Crossing的网络结构图示如下：

![](https://tva1.sinaimg.cn/large/007S8ZIlly1ghr7k75kw9j30f10be0sp.jpg)

其主要包括四层：

1)Embedding层：将经过one-hot之后的稀疏的类别型特征向量转换成低维稠密的Embedding向量；而数值型特征无需经过Embedding层,直接进入下一层。

2)Stacking层：将不同的Embedding特征和数值型特征拼接在一起,形成新的包含全部特征的特征向量。

3)Multiple Residual Units层：多层残差网络对各个维度进行充分的交叉组合,使得模型可以抓取到更多的非线性特征和组合特征的信息,提高模型的表达能力。

4)Scoring层：对于CTR预估二分类,scoring层往往使用的是逻辑回归模型,而对于多分类问题,往往使用softmax模型。



#### Deep Crossing中的残差单元有什么作用？

残差单元的结构如下,其输入一方面经过两层以ReLU为激活函数的全连接层,生成输出向量,另一方面可以通过一个短路(shortcut)通路直接与输出向量进行元素加,生成最终的输出向量。所以残差单元中两层的ReLU拟合的其实是输出和输入之间的"残差"。

![](https://tva1.sinaimg.cn/large/007S8ZIlly1ghr7kytl92j30ap07iglg.jpg)
残差单元主要为了解决两个问题：

1)随着网络加深,神经网络往往存在过拟合现象,而在残差单元中,由于有输入向量短路的存在,可以越过两层ReLU网络,减少过拟合现象的发生。

2)当神经网络层数过多,往往会产生严重的梯度消失现象。残差单元中的ReLU激活函数可以有效缓解梯度消失,而且输入向量短路相当于直接把梯度毫无变化地传递到下一层,也使得残差网络的收敛速度更快。

## NeuralCF

#### NeuralCF在矩阵分解模型的基础上,做了哪些改进？
NeuralCF用“多层神经网络+输出层”的结构替代了矩阵分解模型中的简单内积操作


其中原始的矩阵分解使用内积让用户与物品交互,为了进一步让向量在各维度上进行充分交叉,可以通过“元素积”(两个长度相等的向量对应维度相乘得到另一个向量)的方式进行互操作,这是广义的互操作形式。

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghr7vee5oaj30f60bh0xi.jpg)

NeuralCF混合模型整合了原始NeuralCF模型和以元素积为互操作的广义矩阵分解模型,使得模型具有了更强的特征组合和非线性能力。

但由于NeuralCF是基于协同过滤的思想进行改造的,并没有引入更多其他类型的特征,这无疑损失了很多信息


#### 模型结构越复杂越好吗？特征越多越好吗？

并不是模型越复杂、特征越多越好。首先要防止过拟合的风险,然后往往需要更多的数据和更长的训练时间才能使得复杂的模型收敛,需要在模型的实用性、实时性和效果之间进行权衡。

## PNN模型

#### PNN相较NeuralCF和Deep Crossing有哪些改进？优势是什么？

相比NeuralCF,PNN模型的输入不仅包括用户和物品信息,还可以有更多不同形式、不同来源的特征,通过Embedding层的编码生成同样长度的稠密特征Embedding向量。

相比Deep Crossing,在输入、Embedding层、多层神经网络,以及最终的输出层没哟偶结构上的不同,唯一的区别在于PNN用乘积层代替了Deep Crossing模型的Stacking层,不同的Embedding向量不再是简单的拼接,而是用Product操作进行两两交互,更有针对性地获取特征之间的交叉信息。

#### PNN中特征交叉有哪些方式？分别是怎么样的？

PNN引入了乘积层对特征进行交叉,它由线性操作部分和乘积操作部分组成,其中,乘积特征交叉部分又分为内积操作和外积操作,使用内积操作的PNN模型被称为IPNN,使用外积操作哦的PNN模型被称作OPNN。

内积操作就是经典的向量内积,对应位置相乘组成一个新的向量；

外积操作是对特征向量的各维度进行两两交叉,生成一个特征交叉方形矩阵。外积操作会将复杂度从M提升到$M^2$(M是输入向量的维度),为了在一定程度上减小复杂度,PNN通过把所有两两特征Embedding向量外积互操作的结果叠加,也即先通过一个平均池化层,再进行外积互操作。但对具有不同含义的特征强行池化,会模糊很多信息。


#### PNN模型的优势和局限性是什么？

交叉方式的多样化,相比于简单的交由全连接层进行无差别化的处理,PNN中的内积和外积操作更有针对性地强调了不同特征之间的交叉,让模型更容易捕获特征的交叉信息。

但PNN中的外积操作,为了优化训练效率,对所有的特特征进行无差别的交叉,在一定程度上忽略了原始特征向量中包含的有价值信息。

## Wide&Deep

#### 如何理解Wide&Deep模型的泛化能力和记忆能力？

记忆能力：记忆能力可以被理解模型直接学习并利用历史数据中物品或者特征的“共现频率”的能力。像逻辑回归、协同过滤等简单模型的结构较为简单,原始数据往往可以直接影响推荐结果,产生类似于“如果点击过A,就推荐B”这类规则式的推荐,相当于模型直接记住了历史数据的分布特点,并利用这些记忆进行推荐。即一旦发现了“强特征”,相应的权重会在模型(例如逻辑回归)训练过程中被调整的非常大,就实现了对这个特征的记忆。

泛化能力：可以被理解成模型传递特征的相关性,以及挖掘稀疏甚至从未出现过的稀有特征与最终标签相关性的能力。深度神经网络模型(称为 deep 模型)只需要执行较少的特征工程即可泛化到未出现的特征组合。

Wide&Deep模型的提出不仅综合了“记忆能力”和“泛化能力”,而且开启了不同网络结构融合的新思路。
#### 请简述Wide&Deep的模型结构。
Wide&Deep包含一个 linear model:LM 部分和一个 neural network:NN 部分。

其中单层的linear model即wide部分擅长处理大量稀疏类id特征；Deep部分利用神经网络表达能力强的特点,进行深层交叉,挖掘藏在特征背后的数据模式。

设模型的输入特征向量为$\overrightarrow {x} = (x_1,..x_d)$  是一个$d$维的特征向量(经过 one-hot ),仅包含原始特征。$\phi(·)$表示特征交叉转换函数, $\phi(\overrightarrow x)$包含转换后的特征。

LM 部分：即左侧的 wide 子模型,它是一个线性模型：
$$y=w·<\overrightarrow {x},\phi(\overrightarrow x)>+b$$
其中$<>$表示特征拼接,$w$是模型参数,$b$ 为偏置。

NN 部分：即右侧的 deep 子模型,它是一个 DNN 模型。

输入层：为了缓解模型的输入大小,DNN 的所有离散特征的输入都是原始特征,而没有经过 one-hot 编码转换。

第一层 embedding 层：将高维稀疏的 categorical 特征转换为低维的 embedding 向量。论文中的embedding 向量维度为 32 维。

第二层特征拼接层：将所有的 embedding 向量拼接成一个 dense feature 向量。论文中该向量维度为 1200维。

后续每一层都是全连接层：
$$\overrightarrow h^{(l+1)} = \sigma(w·\overrightarrow h^{(l)}+b)$$
其中$l$为层的编号,$\sigma$为激活函数。

模型联合了 wide 和 deep 的输出：
$$\hat y = sigmoid(w_{wide}<\overrightarrow {x},\phi(\overrightarrow x)>+w_{deep }\overrightarrow h^{(l)}+b)$$
其中  为 wide 部分的权重,  为 deep 部分的权重,  为全局偏置。

模型的损失函数为负的对数似然,并通过随机梯度下降来训练.

![](https://tva1.sinaimg.cn/large/007S8ZIlly1ghrbmd81n3j30xc0800t5.jpg)

#### Wide&Deep模型的创新和优势是什么？

Wide&Deep应用于多家一线互联网公司,其后续的改进创新工作也一直在持续,DeepFM、NFM等模型都可以看成Wide&Deep模型的延伸。

1)它抓住了业务的本质,能够融合传统模型记忆能力和深度学习模型泛化能力的优势

2)模型的结构不复杂,比较容易在工程上实现、训练和上线,加速了在工业界的应用。

## Deep&Cross模型

#### Deep&Cross相比Wide&Deep作了哪些改进？Deep&Cross模型的Cross网络是怎么操作的？

Deep&Cross的主要思路是使用 Cross网络替代原来的Wide部分,从而增加特征之间的交互力度。模型结构如下,
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghrduj7h12j30l50kht99.jpg) 


位于模型结构图左侧的Cross网络使用多层交叉层对输入向量进行特征交叉。假设第$l$层交叉层的输出向量为$x_l$,那么第$l+1$层的输出向量如下：

$$x_{l+1}=x_0x_l^TW_l+b_l+x_l$$
![](https://tva1.sinaimg.cn/large/007S8ZIlly1ghrdu00rwtj30e005l3yg.jpg)
在每一层的都保留了输入向量,且增加了一个n维的权重向量$wl$,在Wide&Deep模型中Wide部分的基础上进行特征的自动化交叉,避免了更多基于业务理解的人工特征组合。
## FNN模型

#### FNN模型的提出主要是为了解决什么问题？它是如何解决的？

在神经网络的参数初始化过程中,往往采用不带有先验信息地随机初始化,但由于Embedding层的输入极端稀疏化,在随机梯度下降的过程中,只有与非零特征相连的Embedding层的权重会被更新,导致Embedding层的收敛速度很慢,且Embedding层的参数数量往往占整个神经网络参数数量的大半以上,模型的收敛速度往往受限于Embedding层。

针对这个难题,FNN使用FM模型训练好的各特征隐向量初始化Embedding层的参数,相当于在初始化神经网络参数时引入了先验信息,这样一来神经网络训练的起点更接近于目标的最优点,加速了神经网络的收敛。

#### FNN的模型结构是什么样的？


#### FNN模型中是如何使用FM来初始化Embeddiing层参数的？

## DeepFM

#### DeepFM相较于Wide&Deep有什么改进？为什么这么改进？

DeepFM对Wide&Deep模型的改进之处在于用FM替换了原来的Wide部分,加强了浅层网络部分特征组合的能力。DeepFM中的FM层和隐藏层共享输入。这种共享输入使得DeepFM可以同时从原始特征中学习低阶特征交互和高阶特征交互,完全不需要执行特征工程。

![](https://tva1.sinaimg.cn/large/007S8ZIlly1ghseamfau3j30k80a7goq.jpg)


## NFM

#### NFM相比Wide&Deep有什么改进？为什么这么改？

NFM使用了一个表达能力更强的函数来替代Wide&Deep中的深层完了,具体的,他在Embedding层和多层神经网络层之间加入了特征交叉池化层(Bi-Interaction层),具体的结构如下：

![](https://tva1.sinaimg.cn/large/007S8ZIlly1ghseoemot0j30ev0aq3yg.jpg)

其中Bi-Interaction层对输入的$v_x$执行池化操作,将一组 embedding 向量(即一个矩阵)转换为一个向量,该操作称作 Bi-Interaction pooling 操作：

$$f_{BI}(v_x)=\sum_{i=1}^n\sum_{j=i+1}^n(x_iv_j)(x_jv_j)$$

其中：  是逐元素乘法；$f_{BI}(v_x)$ 是一个$k$维向量,它在 embedding 空间编码了二阶交叉特征。

Bi-Interaction 层有两个优秀的特性：

- Bi-Interaction 层并没有引入任何额外的模型参数
- Bi-Interaction 层可以在线性时间内有效计算：

$$f_{BI}(v_x) = \frac{1}{2}[(\sum_{i=1}^n x_iv_i)^2-\sum_{i=1}^n(x_iv_i)^2 ]$$


由于输入$x$的稀疏性,我们可以在$O(kn_x)$的时间内计算 Bi-Interaction pooling ,其中$n_x$表示输入中非零项的数量。

这些性质意味着 Bi-Interaction 层对二阶交叉特征建模的代价很低。
## AFM

#### AFM的网络结构是什么样的？

与 Wide&Deep 以及 DeepCross 等模型相比,AFM 结构简单、参数更少、效果更好。同时 AFM 具有很好的可解释性：通过注意力权重可以知道哪些交叉特征对于预测贡献较大。


#### AFM模型为何要在推荐系统模型中引入注意力机制？是如何引入注意力机制的？

## DIN

#### DIN的结构是怎样的？是如何引入注意力机制的？动机是什么？

## DIEN

#### DIEN引入序列信息的动机是什么？

#### 请绘制DIEN各层的结构？兴趣抽取层和兴趣进化层。

## ESSM

#### 请说明ESSM模型的多目标优化设计的动机,以及是怎么做的？


