---
layout: post
title: 推荐系统百问百答 | Embedding篇
date: 2021-02-20 20:20:28
categories: 
        - 推荐系统百问百答
        - Embedding篇
tags:
        - Word2Vec
        - Item2Vec
        - DeepWalk
        - Node2Vec
        - LINE
        - EGES
        - SDNE
        - Struct2vec
        - 
featured: true
math: true
thumbnail: 
lede:  
---

![](https://tva1.sinaimg.cn/large/008eGmZEly1gnsod9ntxtj31c40qmwfu.jpg)

## Embeddng

#### Embedding技术为何对深度学习推荐系统来说非常重要？

1）推荐场景中大量使用one-hot编码对类别、id型特征进行编码，导致样本特征向量极度稀疏，而深度学习的结构特点使其不利于稀疏特征向量的处理，因此几乎所有深度学习推荐模型都会由Embedding层负责将高维稀疏特征转换为低维稠密特征向量。因此，各类Embedding技术是构建深度学习推荐模型的基础性操作。

2）Embedding本身就是极其重要的特征向量。相比MF等传统方法产生的特征向量，Embedding的表达能力更强，特别是Graph Embedding技术被提出后，Embedding几乎可以引入任何信息进行编码，使其本身就包含大量有价值的信息。在此基础上，Embedding向量往往会与其他推荐系统特征连接后一同输入后续深度学习网络进行训练。

3）Embedding对物品、用户相似度的计算是常用的推荐系统召回层技术。在局部敏感哈希（Locality-Sensitive Hashing）等快速最近邻搜索技术应用于推荐系统后，Embedding更适用于对海量备选物品进行快速“初筛”，过滤出几百到几千的物品交由深度学习网络进行“精排”。


#### 请简述Word2Vec的原理和结构。

#### 为了加快Word2Vec的训练，采取了什么方法？

#### 谈谈你对Item2Vec的理解，它的局限是什么？

#### 请简述双塔模型的结构，其中物品塔的作用是什么？

#### DeepWalk的主要思想是什么？说出算法步骤

在互联网场景下，数据对象之间更多呈现的是图结构，比如由用户行为数据生成的物品关系图，以及属性与实体组成的知识图谱。Graph Embedding是一种对图结构中的节点进行Embedding编码的方法，最终生成的节点Embedding向量一般包含图的结构信息及附近节点局部相似性信息。

DeepWalk是影响力比较大的Graph Embedding方法，主要思想是在由物品的图结构上随机游走，产生大量物品序列，然后将这些物品序列作为训练样本输入Word2Vec进行训练，得到物品的Embedding。

它的步骤如下：
![](https://tva1.sinaimg.cn/large/007S8ZIlly1gi5hv31x42j31580c4wia.jpg)

1）a是原始的用户行为序列
2）基于用户行为序列构建物品关系图，比如物品A和B之间的边产生的原因是用户U1先后购买了物品A和物品B,若后续产生了多条相同的有向边，则有向边的权重被加强，在将所有用户行为序列都转换成物品关系图中的边之后，全局的物品关系图就建立起来了。
3）采用随机游走的方式随机选择起始点，重新产生物品序列
4）将这些物品序列输入到d所示的Word2Vec模型中，生成最终的物品Embedding向量。



#### Node2Vec中的同质性和结构性指的是什么？他们与DFS和BFS的对应关系如何？

Node2Vec通过调整随机游走权重的方法使Graph Embedding的结果更倾向于体现网络的同质性或结构性。

网络的”同质性“指的是距离相近节点的Embedding应尽量近似，如图所示，节点$u$与其相连的节点$s_1$、$s_2$、$s_3$、$s_4$的Embedding表达应该是接近的，这就是网络”同质性“的体现。

”结构性“指的是结构上相似的节点的Embedding应尽量近似，图中节点U和节点$s_6$都是各自局域网络的中心节点，结构上相似，其Embedding的表达也应该近似，这就是”结构性“的体现。

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gi5jakfjb9j30kq072gm3.jpg)

为表达同质性，可以让游走的过程更倾向于DFS，因为DFS更有可能通过多次跳转，游走到远方的节点上，但无论怎样，DFS的游走更大概率会在一个集团内部进行，这就使得一个集团或者社区内部的节点的Embedding更为相似，从而更多地表达网络的”同质性“。

为表达”结构性“，可以让游走的过程更倾向于BFS，因为BFS会更多地在当前节点的领域中游走遍历，相当于对当前节点周边的网络结构进行一次”微观扫描“。当前节点是"局部中心节点"，还是”边缘节点“，或是”连接性节点“，其生成的序列包含的节点数量和顺序必然是不同的，从而最终的Embedding抓取到更多结构性信息。

#### 请写出Node2Vec的节点间跳转概率公式。

在Node2Vec算法中，主要是通过节点间的跳转概率来控制其同质性和结构性的倾向的。

下图所示为Node2Vec算法从节点t跳转到节点v，再从节点v跳转到周围各点的跳转概率。

从节点v跳转到下一个节点x的概率$\pi_{px}=\alpha_{pq}(t,x)·w_{vx}$，其中$w_{vx}$是边$vx$的权重，$\alpha_{pq}(t,x)$的定义如下：

$$\begin{cases}  \frac{1}{p},  if d_{tx}=0 \\ 1,if d_{tx}=1 \\ \frac{1}{q} ,if d_{tx}=2\end{cases}$$

其中，$d_{tx}$指节点$t$到节点$x$的距离，参数$p$和$q$共同控制随机游走的倾向。参数$p$被称为返回参数，$p$越小，随机游走回节点$t$的可能性越大，Node2Vec就更注重表达网络的结构性。参数$q$被称为进出参数，$q$越小，随机游走到远方节点的可能性越大，Node2Vec就更注重表达网络的同质性；反之，则当前节点更可能在附近节点游走。



#### 举例说明Node2Vec的同质性和结构性在推荐系统中的直观解释。

同质性相同的物品很可能是同品类、同属性，或者经常被一同购买的商品，而结构性相同的物品则是各品类的爆款、各品类的最佳凑单商品等拥有类似趋势或者结构性属性的商品。这两者在推荐系统中都是非常重要的特征表达。

由于Node2Vec的这种灵活性，以及发掘不同图特征的能力，甚至可以把不同Node2Vec生成的偏向”结构性“的Embedding结果和偏向”同质性“的Embedding结果共同输入后续的深度学习网络，以保留物品的不同图特征信息。

#### LINE

#### EGES的提出主要是为了弥补DeepWalk的缺陷的？怎么弥补的？

单纯使用用户行为生成的物品相关图，固然可以生成物品的Embedding，但如果遇到新加入的物品，或者没有过多互动信息的”长尾“物品，则推荐系统将出现严重的冷启动问题。为了使”冷启动“的商品获得”合理“的初始Embedding,EGES通过引入更多补充信息（side information）来丰富Embedding信息的来源，从而使得没有历史行为记录的商品获得较合理的初始Embedding。

#### 请简述EGES模型的结构，并简述每一层的做法。

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gi6ja4uh51j30ex0ai787.jpg)
如上图所示，在深度神经网络中加入平均池化层，将不同Embedding平均起来。为了防止简单的平均池化导致有效Embedding的丢失，EGES对每个Embedding加上了权重，即对每一类Embedding向量，分别赋予权重$a_0,a_1,···,a_n$，图中的隐层表达就是对不同Embedding进行加权平均操作的层，然后输入到softmax层，通过梯度反向传播，求得每个Embedding的权重。在实际模型中，权重采用了$e^{a_j}$而不是$a_j$，一方面可以避免权重为0，另一方面因为$a^{a_j}$在梯度下降过程中有良好的数学性质。



#### SDNE

#### Struct2Vec

#### Embedding在深度学习推荐系统中有哪些应用？列举三个方向。

1)在深度学习网络中作为Embedding层，完成从高维稀疏特征向量到低维稠密特征向量的转换。

2）组我诶预训练的Embedding特征向量，与其他特征向量连接后，一同输入深度学习网络进行训练。

3）通过计算用户和物品的Embedding相似度，Embedding可以直接作为推荐系统的召回层或者召回策略之一。


#### Embedding作为深度学习模型的训练会存在什么问题？

将Embedding层与整个深度学习网络整合后一同进行训练是理论上最优的选择，因为上层梯度可以直接反向传播到输入层，模型整体是自洽的。但Embedding层的参数数量巨大，因此Embedding层的加入会拖慢整个神经网络的收敛速度，所以很多工程上要求快速更新深度学习推荐系统放弃了Embedding层端到端训练，而用预训练Embedding层的方式替代。

#### Embedding有哪些预训练方法，分别介绍一下.

1）通过FM模型训练得到的各特征隐向量作为Embedding层的初始化权重，例如FNN模型就是使用该预训练方法。更甚，可以直接在FNN模型的训练过程中，固定预训练好的Embedding层，而仅更新上层神经网络权重，这是更彻底的Embedding预训练方法。

2）Embedding的本质是建立高维到低维向量的映射，而映射方法并不局限于神经网络，可以是任何异构模型。比如GBDT+LR模型，其中GBDT部分在本质上就是进行了一次Embedding操作，利用GBDT模型完成Embedding预训练，再将Embedding输入单层神经网络进行CTR预估。

3）Graph Embedding技术使得Embedding本身的表达能力进一步增强，而且能够将各类补充信息全部融入到Embedding中，使Embedding成为非常有价值的推荐系统特征。

#### 请简述Embedding作为召回层的过程。
以YoouTube推荐系统召回层为例，结构图如下：
![](https://tva1.sinaimg.cn/large/007S8ZIlly1ghxcpxr7lqj30k00fr74x.jpg)

模型的输入层特征全部是用户相关特征，从左往右依次是用户观看历史视频的Embedding向量，用户搜索词Embedding向量、用户地理属性特征Embedding向量、用户年龄、性别相关特征。

模型的输出层是softmax层，该模型本质上是一个多分类模型，预测目标是用户观看了哪个视频，因此softmax层的输入是经过三层ReLu全连接层生产的用户Embedding，输出向量是用户观看每一个视频的概率分布。由于输出向量的每一维对应了一个视频，该维对应的softmax层列向量就是物品Embedding。通过模型的离线训练，可以最终得到每个用户的Embedding和物品Embedding。

模型部署时，没有必要部署整个深度神经网络来完成从原始特征向量到最终输出的预测过程，只需要将用户Embedding和物品Embedding存储到内存数据库，通过内积运算再排序的方法就可以得到物品的排序，再通过取序列中TopN的物品即可得到召回的候选集合，这就是利用Embeddng作为召回层的过程。

但因为候选集量级巨大，内积运算会消耗大量计算时间，导致线上推断延迟，这时需要使用局部敏感哈希让Embedding进行快速搜索。

#### 请简述局部敏感哈希的原理及其在推荐系统中的作用。


