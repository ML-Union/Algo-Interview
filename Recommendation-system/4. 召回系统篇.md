![](https://tva1.sinaimg.cn/large/008eGmZEly1gnsohni889j31c00qmdh6.jpg)

## 召回

#### 假设物品库数量达到百万级别，如何设计方法从这个数量级别的物品中推荐给用户top10的物品，同时可以减少计算的压力？

在推荐系统的排序层，一般会使用复杂模型，利用多特征进行精准排序，而如果直接对百万量级的候选集进行逐一推断的话，则计算资源和延迟都是在线服务过程无法忍受的。

因此可以加入召回过程，利用少量的特征和简单的模型或规则进行候选集的快速筛选，减少精准排序阶段的时间开销。

#### 推荐系统中为什么要有召回？在推荐系统中召回和排序有什么异同？

首先，在实际应用中，推荐算法往往是在线上使用，可用的设备资源和响应时间都是有限的。而整个物品集的规模往往十分庞大，在线上对大量物品进行排序是对性能的较大挑战，很难实现。召回可以视为一个粗排序的过程，这个过程的主要目的是在有限的资源条件下提供尽可能准确的一个小候选集，从而减轻排序阶段的计算负担和耗时。

其次，即使资源和时间足够对整个物品集进行扫描，先使用较为简单的方法进行召回往往也是比较有利的，先进行召回意味着可以排除大部分无关物品，从而允许在排序阶段对更小的候选集使用更多的特征和更复杂的模型，以提高排序的准确率。

召回层一般待计算的候选集合大、速度快、模型简单、特征较少，尽量让用户感兴趣的物品在这个阶段能够快速被召回，即保证相关物品的召回率。工业界主流的召回方式是采用多路召回策略，采用多个简单策略叠加的方式得到最终的召回结果。

排序层的首要目标是得到精准的排序结果，需处理的物品数量少，可利用多特征，使用比较复杂的模型。


#### 请举例说明一些简单的多路召回策略。

协同过滤召回：

热门召回：

兴趣召回：

流行度召回



#### 请简述基于embedding的召回方法，优势是什么？

在YouTube推荐系统中利用深度学习网络生成Embedding作为召回层的方法，再加上可使用局部敏感哈希进行快速的Embedding的召回方法在效果和速度上均不逊色于多路召回。

而且，多路召回中使用”兴趣标签“、”热门度“、”流行趋势“、”物品属性“等信息都可以作为Embedding召回方法中的附加信息（side information）融合进最终的Embedding向量中，就相当于在利用Embedding召回的过程中，考虑到了多路召回的多种策略。

Embedding召回的另一个优势在于评分的连续性，多路召回中不同召回策略产生的相似度、热度等分值并不具备可比性，无法据此决定每个召回策略放回候选集的大小，Embeddding召回可以把Embedding召回可以把Embedding间的相似度作为唯一的判断标准，因此可以随意限定召回的候选集大小。

#### 请简要描述DSSM是如何应用于召回的？结构是怎样的？

DSSM模型是微软2013年发表的一个关于query/ doc的相似度计算模型，后来发展成为一种所谓”双塔“的框架广泛应用于广告、推荐等领域的召回和排序问题中。网络结构如下：

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gi6las6ozrj30im08jq5m.jpg)

1）首先特征输入层将Query和Doc（one-hot编码）转化为embedding向量，原论文针对英文输入还提出了一种word hashing的特殊embedding方法用来降低字典规模。我们在针对中文embedding时使用word2vec类常规操作即可；

2）经过embedding之后的词向量，接下来是多层DNN网络映射得到针对Query和Doc的128维语义特征向量；

3）最后会使用Query和Doc向量进行余弦相似度计算得到相似度R，然后进行softmax归一化得到最终的指标后验概率P，训练目标针对点击的正样本拟合P为1，否则拟合P为0；

DSSM 模型的最大特点就是 Query 和 Document 是两个独立的子网络，后来这一特色被移植到推荐算法的召回环节，即对用户端（User）和物品端（Item）分别构建独立的子网络塔式结构。该方式对工业界十分友好，两个子网络产生的 Embedding 向量可以独自获取及缓存。目前工业界流行的 DSSM 双塔网络结构如下图所示：

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gi6l79v8xnj30fh0ai3zx.jpg)

双塔模型两侧分别对（用户，上下文）和（物品）进行建模，并在最后一层计算二者的内积。

其中：

- $x$为（用户，上下文）的特征，$y$为（物品）的特征；
- $u(x)$表示（用户，上下文）最终的 Embedding 向量表示， $v(y)$ 表示（物品）最终的 Embedding 向量表示；
- $<u(x),v(y)>$ 表示（用户，上下文）和（物品）的余弦相似度。

当模型训练完成时，物品的 Embedding 是可以保存成词表的，线上应用的时候只需要查找对应的 Embedding 即可。因此线上只需要计算 （用户，上下文） 一侧的 Embedding，基于 Annoy 或 Faiss 技术索引得到用户偏好的候选集。


#### 请简述YoutubeDNN的结构和其原理

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gi6ll1wlb3j310i0u0tfe.jpg)

特征输入层包含了三部分内容：用户观看过的 video 的 embedding 向量、用户搜索词的 embedding 向量以及用户的地理位置年龄等静态特征；这里的 embedding 向量作者是用 word2vec 类方法预先生成的。

线下模型训练阶段，三层 ReLU 神经网络之后接到 softmax 层，也就是说在这里建模为为用户推荐下一个感兴趣视频的多分类问题，输出是在所有候选视频集合上的概率分布。

线上预测阶段，考虑到召回的高性能需求首先通过 userId 找到相应的用户向量，然后使用 KNN 类方法找到相似度最高的 N 条候选结果返回。

模型整体的思路可以认为是传统协同过滤思路的扩展。传统的User-cf强调相似的用户感兴趣的物品也相似；传统的Item-cf强调对物品A感兴趣的用户，可能也对物品A相似的物品同样感兴趣。所以传统的User-cf和Item-cf实际上是分别构造了用户向量空间和物品向量空间，在任何一个向量空间找到相似性都可以进行推荐。

而 YoutubeDNN 则学习统一的（用户、物品）向量空间来代替原来的两个独立的向量空间，使用深度网络将用户、物品映射到这个统一的低维向量空间来发现学习更高阶的用户物品相似性。


#### 请简述用户多兴趣网络MIND的出发点是什么，描述其结构？

电商场景下用户行为序列中的兴趣分布是多样的，如下图用户 A 和 B 的点击序列商品类别分布较广，因此如果只用一个 embedding 向量来表示用户的兴趣其表征能力是远远不够的。所以需要通过一种模型来建模出用户多个 embedding 的表示。MIND 模型通过引入 capsule network 的思想来解决输出多个向量 embedding 的问题，具体结构如下图：

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gi6n593dj8j30u00dw0yx.jpg)


Multi-Interest 抽取层负责建模用户多个兴趣向量 embedding，然后通过 Label-aware Attention 结构对多个兴趣向量加权。这是因为多个兴趣 embedding 和待推荐的 item 的相关性肯定不同 ( 这里的思想和 DIN 模型如出一辙 )。其中上图中的 K，V 都表示用户多兴趣向量，Q 表示待推荐 item 的 embedding 表示，最终用户的 embedding 表示为：
![](https://tva1.sinaimg.cn/large/007S8ZIlly1gi6n8zzdvuj30i604k3zx.jpg)

公式中的 $e_i$ 表示 item embedding，$V_u$ 表示 Multi-Interest 抽取层输出的用户多个兴趣向量 embedding。

然后使用 $V_u$ 和待推荐 item embedding，计算用户 u 和商品 i 交互的概率，计算方法和 YouTube DNN 一样：

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gi6nbpe9amj30nx05dwg2.jpg)

#### SDM是如何结合用户长期和短期兴趣建模的？其结构如何？

在电商场景中，用户都会有短期兴趣和长期兴趣，比如在当前的浏览 session 内的一个点击序列，用户的需求往往是明确的，这属于用户短期的兴趣。另外用户还有一些长期的兴趣表达，比如品牌、店铺的偏好。因此通过模型分别建模用户的长、短期兴趣是有意义的。

上图中$P_u$表示用户短期兴趣向量$s^u_t$表示用户的长期兴趣向量，这里需要注意的是，在求长期和短期用户兴趣向量时都使用了 Attention 机制，Attention 的 Query 向量$e_u$ 表示 user 的 embedding，用的是基本的用户画像，如年龄区间、性别、职业等。得到长期和短期用户向量后，通过 gru 中的 gate 机制去融合两者：

$$o^u_t = (1-G_t^u) a \times bp^u+G_t^u s_t^u$$

上面的公式输出表示用户的 embedding 表示，而 item 的 embedding 表示和 YouTube DNN 一样，可以拿 softmax 层的权重。其实也可用 graph embedding 先离线训练好 item 的 embedding 表示。

线上预测：通过user id找到相应的user embedding，然后使用 KNN 方法 (比如faiss)找到相似度最高的 top-N 条候选结果返回。

#### TDM 深度树匹配召回
其基本原理是使用树结构对全库 item 进行索引，然后训练深度模型以支持树上的逐层检索，从而将大规模推荐中全库检索的复杂度由 O(n) ( n 为所有 item 的量级 ) 下降至 O(log n)。


